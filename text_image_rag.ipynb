{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4f0381",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (3.11.2) (Python 3.11.2)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/USER/Desktop/my project/text_image_rag/venv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "from langchain_core.documents import Document\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from PIL import Image\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import base64\n",
    "import io\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6cd4a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "str expected, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      3\u001b[39m load_dotenv()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOPENAI_API_KEY\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m clip_model=CLIPModel.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mopenai/clip-vit-base-patch32\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m clip_processor=CLIPProcessor.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mopenai/clip-vit-base-patch32\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:684\u001b[39m, in \u001b[36m__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:744\u001b[39m, in \u001b[36mcheck_str\u001b[39m\u001b[34m(value)\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: str expected, not NoneType"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"Groq_API_key\"]=os.getenv(\"Groq_API_key\")\n",
    "\n",
    "clip_model=CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor=CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed066643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_image(image_data):\n",
    "    if isinstance(image_data, str):\n",
    "        image= image.open(image_data).convert(\"RGB\")\n",
    "    else:\n",
    "        image= image_data\n",
    "    \n",
    "    inputs=clip_processor(images = image, return_tensors = \"pt\")\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "        features= features/features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().numpy()\n",
    "    \n",
    "def embed_text(text):\n",
    "    inputs=clip_processor(\n",
    "        images = image,\n",
    "        return_tensors = \"pt\",\n",
    "        padding=True,\n",
    "        transaction = True,\n",
    "        max_length = 77\n",
    "        )\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "        features= features/features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5235216f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "no such file: 'sample.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_15128\\372793747.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m pdf_path=(\u001b[33m\"sample.pdf\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m doc = fitz.open(pdf_path)\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m all_docs=[]\n\u001b[32m      5\u001b[39m all_emdeddings=[]\n",
      "\u001b[32mc:\\Users\\USER\\OneDrive\\Desktop\\my project\\text_image_rag\\venv\\Lib\\site-packages\\pymupdf\\__init__.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[39m\n\u001b[32m   3024\u001b[39m                     self.page_count2 = extra.page_count_pdf\n\u001b[32m   3025\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3026\u001b[39m                     self.page_count2 = extra.page_count_fz\n\u001b[32m   3027\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3028\u001b[39m             JM_mupdf_show_errors = JM_mupdf_show_errors_old\n",
      "\u001b[31mFileNotFoundError\u001b[39m: no such file: 'sample.pdf'"
     ]
    }
   ],
   "source": [
    "pdf_path=(\"sample.pdf\")\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "all_docs=[]\n",
    "all_embeddings=[]\n",
    "image_data_stores={}\n",
    "\n",
    "splitter= RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,page in enumerate(doc):\n",
    "    text=page.get_text()\n",
    "    if text.strip():\n",
    "        temp_data=Document(page_content=text, metadata={\"page\":i, \"type\":'text'})\n",
    "        text_chunks=splitter.split_documents({temp_data})\n",
    "\n",
    "        for chunk in text_chunks:\n",
    "            embedding= embed_text(chunk.page_content)\n",
    "            all_embeddings.append(embedding)\n",
    "            all_docs.append(chunk)\n",
    "        \n",
    "\n",
    "        for img_index, img  in enumerate(page.get_images(full=True)):\n",
    "            try:\n",
    "                xref=img[0]\n",
    "                base_image=doc.extract_image(xref)\n",
    "                image_bytes=base_image[\"image\"]\n",
    "\n",
    "                pil_image= Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "                image_id=f\"page_{i}_img_{img_index}\"\n",
    "\n",
    "                buffered = io.BytesIO()\n",
    "                pil_image.save(buffered, format=\"PNG\") \n",
    "                img_base64=base64.b64encode(buffered.get_value()).decode()\n",
    "                image_data_stores[image_id]= img_base64\n",
    "\n",
    "                embedding = embed_image(pil_image)\n",
    "                all_embeddings.append(embedding)\n",
    "\n",
    "\n",
    "                image_doc = Document(\n",
    "                    page_content = f\"[Image:{image_id}]\"\n",
    "                    metadata = {\"page\": i, \"type\":\"image\", \"image_id\": image_id}\n",
    "                )\n",
    "                all_docs.append(image_doc)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_index} on page {i}: {e}\")\n",
    "                continue\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5a1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##all_embeddins\n",
    "##all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628b7ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = np.array(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a908341",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store =  FAISS.from_embeddings (\n",
    "    text_embeddings = [(doc.page content, emb) for doc, emb in zip (all_docs, embeddings_array)],\n",
    "    embedding= None,\n",
    "    metadatas=[doc.metadata for doc in all_docs]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2f5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"Groq_API_key\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c261e973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_multimodal(query, k=5):\n",
    "    \"\"\"Unified retrieval using embeddings for both text and images\"\"\"\n",
    "    query_embedding = embed_text(query)\n",
    "    results = vector_store.similarity_search_by_vector(\n",
    "        embedding= query_embedding,\n",
    "        k=k\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237285c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_message(query, retrieved_docs):\n",
    "    content=[]\n",
    "    content.append(\n",
    "        \"text\": \"text\",\n",
    "        \"text\": f\"Question:{query}\\n\\nContext:\\n\"\n",
    "    )\n",
    "\n",
    "    text_docs= [doc for doc in retrieved_docs if doc.metadata.get(\"type\")==\"text\"]\n",
    "    image_docs= [doc for doc in retrieved_docs if doc.metadata.get(\"type\")==\"image\"]\n",
    "\n",
    "    if text_docs:\n",
    "        text_context = \"\\n\\n\".join([\n",
    "            f\"[Page {doc.metadata['page']}]: {doc.page_content}\"\n",
    "            for doc in text_docs\n",
    "        ])\n",
    "        content.append({\n",
    "            \"type\": \"text\"\n",
    "            text: f\"Text excerpts:\\n{text_context}\\n\"\n",
    "        })\n",
    "    for doc in image_docs:\n",
    "        image_id=doc.matadata.get(\"image_id\")\n",
    "        if image_id and image_id in image_data_stores:\n",
    "            content.append([\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"\\n[Image from page:{doc.metadata['page']}]:\\n\"\n",
    "            ])\n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": f\"data:image/png;base64,{image_data_stores[image_id]}\"\n",
    "                })\n",
    "\n",
    "    content.append([\n",
    "        \"type\":\"text\",\n",
    "        \"text\":\"\\n\\nPlease answer the question based on the provided text and images.\"\n",
    "        ])\n",
    "\n",
    "    return HumanMessage(content=content)\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3734dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_pdf_rag_pipeline(query):\n",
    "    context_docs=retrieve_multimodal(query, k=5)\n",
    "\n",
    "    message=create_multimodal_message(query,context_docs)\n",
    "\n",
    "    response = llm.invoke([message])\n",
    "\n",
    "    print(f\"\\nRetrieved {len(context_docs)} documents:\")\n",
    "\n",
    "    for doc in context_docs:\n",
    "        doc_type = doc.metadata.get(\"type\", \"unknown\")\n",
    "        page = doc.metadata.get(\"type\", \"?\")\n",
    "\n",
    "        if doc_type==\"text\":\n",
    "            preview = doc.page_content[:100] + \"...\" if len(doc.page_content)>100 else doc.page_content\n",
    "            print(f\" - Text from page {page}: {preview}\")\n",
    "        else:\n",
    "            print(f\" - Text from page {page}\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    result response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a3dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    queries=[\n",
    "        \n",
    "    ]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"-\"*50)\n",
    "    answer= multimodal_pdf_rag_pipeline(query)\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
